{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_lab_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Vxs4RaFNJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "#from sklearn import preprocessing, cross_validation\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Orzl6z1D3vC",
        "colab_type": "text"
      },
      "source": [
        "Question 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_PmUj7oyRe3",
        "colab_type": "code",
        "outputId": "24b84035-d196-45f1-8c97-b4aa645adb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "allwalks = []\n",
        "\n",
        "for i in range(250):\n",
        "    randwalk = [0]\n",
        "    for x in range(100):\n",
        "        step = randwalk[-1]\n",
        "        dice = np.random.randint(1,7)\n",
        "        if dice <= 2 :\n",
        "            step = max(0, step - 1)\n",
        "\n",
        "        elif dice<=5:\n",
        "            step += 1\n",
        "\n",
        "        else:\n",
        "            step = step + np.random.randint(1,7)\n",
        "        \n",
        "    print(step)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "5\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "0\n",
            "6\n",
            "0\n",
            "0\n",
            "1\n",
            "6\n",
            "0\n",
            "3\n",
            "4\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "0\n",
            "5\n",
            "1\n",
            "0\n",
            "1\n",
            "5\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "5\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "3\n",
            "3\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "6\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRVdwEaQD7y_",
        "colab_type": "text"
      },
      "source": [
        "Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t52FA7zEASW",
        "colab_type": "text"
      },
      "source": [
        "Generation of Random data for Mutliple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRU9JvfGE0ES",
        "colab_type": "code",
        "outputId": "574be002-374e-43a8-8542-e393539e36ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.310326 -0.538983  0.522009 -0.630752  0.888074\n",
            "1  0.026933  1.005510 -1.519784  0.317596  1.706252\n",
            "2  1.454472 -1.948507  0.989502  1.673824  2.006770\n",
            "3  0.299680 -1.090324 -0.968199  0.285824  0.376355\n",
            "4  1.568637  0.042656 -0.204593  1.126121  2.629879\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.408410  0.615359 -2.553963  1.017630  0.665036\n",
            "96  1.271942  0.739803  1.451475 -2.180999  1.219121\n",
            "97 -1.462029 -1.407781  0.657375  0.320367  0.309101\n",
            "98 -0.355275 -1.795455  0.762725 -0.701842 -0.118037\n",
            "99 -0.845194 -0.817530 -1.009229  0.328676 -0.005031\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.012668   -0.098384    0.002392    0.037210    1.007865\n",
            "std      0.984979    1.056384    0.890788    1.027167    0.875303\n",
            "min     -2.174584   -2.241255   -2.553963   -3.162631   -0.881631\n",
            "25%     -0.761235   -0.900435   -0.565406   -0.652427    0.344661\n",
            "50%     -0.013255   -0.008498   -0.023229    0.078830    1.035789\n",
            "75%      0.724811    0.740714    0.684799    0.733959    1.709912\n",
            "max      2.259437    2.192720    2.373255    2.320635    2.909347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyncXXmRELcZ",
        "colab_type": "text"
      },
      "source": [
        "Data generation for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDCrXiMbFALw",
        "colab_type": "code",
        "outputId": "a70a019d-7da8-47e1-8a31-0e11da956474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "#print(a1)\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "#print(y1)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.310326 -0.538983  0.522009 -0.630752  0.888074\n",
            "1  0.026933  1.005510 -1.519784  0.317596  1.706252\n",
            "2  1.454472 -1.948507  0.989502  1.673824  2.006770\n",
            "3  0.299680 -1.090324 -0.968199  0.285824  0.376355\n",
            "4  1.568637  0.042656 -0.204593  1.126121  2.629879\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.408410  0.615359 -2.553963  1.017630  0.665036\n",
            "96  1.271942  0.739803  1.451475 -2.180999  1.219121\n",
            "97 -1.462029 -1.407781  0.657375  0.320367  0.309101\n",
            "98 -0.355275 -1.795455  0.762725 -0.701842 -0.118037\n",
            "99 -0.845194 -0.817530 -1.009229  0.328676 -0.005031\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.012668   -0.098384    0.002392    0.037210    1.007865\n",
            "std      0.984979    1.056384    0.890788    1.027167    0.875303\n",
            "min     -2.174584   -2.241255   -2.553963   -3.162631   -0.881631\n",
            "25%     -0.761235   -0.900435   -0.565406   -0.652427    0.344661\n",
            "50%     -0.013255   -0.008498   -0.023229    0.078830    1.035789\n",
            "75%      0.724811    0.740714    0.684799    0.733959    1.709912\n",
            "max      2.259437    2.192720    2.373255    2.320635    2.909347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6rOQl9ETi7",
        "colab_type": "text"
      },
      "source": [
        "Data generation for K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZugniEfsFFr2",
        "colab_type": "code",
        "outputId": "6fafb37f-80d5-48c5-ec2e-4013392f7a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAfQElEQVR4nO3dfZCV1X0H8O/v3ru7sqwvERBBXdYYAlIjJiwgTdsMhDSQYp28DUktqSYtjdNkmpl0YqqT0GLLZNpJZjrTjJaJL4lxZDo1DsmqYyS+xVZdlhQcXJQoWdAsAYRYWRbv7t376x+7d3P37vM893k5z8u59/uZccK+8DznYbO/c57f+Z1zRFVBRET2yqXdACIiioaBnIjIcgzkRESWYyAnIrIcAzkRkeUKadx09uzZ2tXVlcatiYistWfPnjdVdU7t51MJ5F1dXejr60vj1kRE1hKRw06fZ2qFiMhyDORERJZjICcishwDORGR5SJPdorIOQCeAdA2cb3/UtUtUa9LRBSHoWIJPfsGMXDyDLpmzcSGpfPR0ZZK3YcxJlpfBLBGVYdEpAXAsyLyqKo+b+DaRNRE4g6yuwdO4cZ7eqEKDI+Mob01j9sf7se9N63A8q4Ljd0naZH/hXR8+8ShiQ9bJv7jlopEFEjcQXaoWMKN9/TiTHFs8nPDI+N/vvGeXvTeuhYzLR2ZG8mRi0heRPYCOA7gcVV9weF7NotIn4j0nThxwsRtiahBVAfZSnAdHhnDmeLYxOdLke/Rs28Qbrt2F0fH8M2d+zFk4D5pMBLIVXVMVa8BcCmAFSJylcP3bFfVblXtnjNn2sIkImpiXkFWFeh5cTDyPQZOnpnsJGqVysDOvYNYuW0Xdg+cinyvpBmtWlHVtwA8CWCdyesSUWPzCrLDI2MYeHM48j26Zs1Ee2ve9eulshp9A0hS5EAuInNE5IKJP88A8BEAL0e9LhE1D68g296aR9fs9sj32LB0PkTqf5+pN4AkmRiRzwPwpIi8CGA3xnPkPQauS9RUhool7Og9gm89egA7eo9Ym68NwyvIigAbrp4f+R4dbQXce9MKzGzLo+AR+Uy9ASTJRNXKiwDeb6AtRE2rkcriwpQQVoJs7b+BCCaCr5lqkuVdF6L31rX45s792Ll3EKXy9MS8qTeAJEkahy93d3crdz8kGjdULGHltl1TyuIqZrblrSqLc+qQKsHYT4d0plhCz4uDGHhzGF2z27Hh6vmxPLut/+YiskdVu2s/zyX6RCmLs2IjyXSNiRLCmW0FbFzeiVvWL8bG5Z2xBdPqNEslN9/emsfMtrzRN4Ck2NVaogZUr2Ljgd7XoYrAqxyTTtf46ZA2Lu80ft+wKmmW6jeA1YsuwhMvH8fPDhyzavl+9ltI1OAqFRtuwXzv62/h4LHTgYJwGqsYkyghNK3yBgCMd3yrv/2UlfMUTK0QpcxPWVzQFEUSC2xqJVFCGJckVpbGiYGcKGVO+Vo3foNwGqPjJEoI45JGx2cSAzlRBlTytVuuW4JrLrvA9fv8BuE0Rsc2TyDamBaqlt1/WaImU8nXqgIHj512DCx+g/CGpfNx+8P9jl+Lc3TsNIEYVwmhSV7zFFlPCwEckRNljokURZqj46RKCE2yOS0EcEEQUSZFXVhTkdQCm0Zg6t88Tm4LghjIiTIqjSDciMeg1fJ6xqx3fAzkROTJhhFpVLY/I5foE5Er2+uo/TD1jFncpTI77wxElBrblteHYeIZs7pLJUfkRGR9HbUfUZ8xy28tDOREFjL9em/z8nq/oj5jlld/MpATWWb3wCms3LYLW3v6cefTh7C1pz/yocG211H7EfUZs/zWwkBOlBF+Rtlxvd7bvLzer6jPmOW3FpYfEmWA37K4Hb1HsOXHL6FYKk+7Rlshh63X/16kScms11GbEPYZs3CqkFv5YWP9hIgsFGTv8IPHTjsGcQAolsr45bGhSG2p3p/bdm4Lf8I+Y1LniobBQE6UsiBlcW8Nj3pe67fDI6abZ6W4ygSzuikYAzlRyoJMol3Q3uJ5rXe1txptWxzi3gYg7tORsvjWwkBOlLIgW6i+d+65aCvkXHPkC+d2xNpWP7wCdRILapphcVMtBnKilAXZO7zyvU7FKYW8pFYmWAnezx06iUf3H0VeBGdHy1MC9ZXzzkvkHNEslAkmvfkYAzlRyoJMomVxwq0yyi6XgbOjlQA6PiSuDtRf++jiREbKaR8SkcYyfgZyogwIMomWpQk3p3y0E1XgiZePJTJS9vuGE8eoOe78vBsGcqKMCDKJlpUJN698dLXxYCZ1R8omgquft5a4Rs1p5ecZyIkoNK98dLX21jw+vHgO+g47byMgAsw7fwZWbttlJLh6vbXEOWpOKz/PQE5EoXnlo6uJAJ9cdhmunH++40j5jhuW4eb79xgNrm5vLWFGzX7fFNLKzzOQE1FoXvloAJjRkkMuJ5MpDbeR8k8STEkEHTUHScMEqUAyiYGciEJzykfPaMmjrIr1V12MVVfMmjYR6zRSTjIlEWTUHDQNk1ZVUeSrishlAH4AYC7Ga462q+q/Rb0uEdnBRBVNkimJIKPmMGmYNKqKTFy5BOCrqvoLETkXwB4ReVxV3d+3iKihRK2i8QquY2XFwWOnsaP3iJESwSCj5rBvCklXFUUO5Kp6FMDRiT+fFpEDAC4BwEBORL44BdfqrQjuenbA6MIav6PmtBcX+WV0P3IR6QLwDICrVPXtmq9tBrAZADo7O5cdPnzY2H2JKFvC1oNX9go/eGwI9z0/gJHS9PiU1N7fQDb2IK/mth+5sROCRKQDwIMAvlIbxAFAVberareqds+ZM8fUbYkoY6IcRVdJSSy8qAOFnHN4SvJ8TFtOTjLSChFpwXgQv19Vf2TimkRkH1OLbbKw8VXFlfPOw9c+ughPvHwcgOCD75mFtkIePztwDK8dH4p9Qyw/TFStCIC7ABxQ1e9EbxIR2crUEvWs5KZra8jbCjk8ffDEZP4+iQ2x/DCRWvkggE0A1ojI3on/PmbgukSUED8HP/tRbyT9QO8RX9ePeuK9CU4HXVcmXyv/a+LgaxNMVK08C8Dln5yIsq521DmjJY8tP34J6666GKvePStQ6qDekv29r/8fDh7rrzuKzcJ2vX43BAPSP7DCaNWKX93d3drX15f4fYkaVdgqEa+qDGDqEns/qYN616vmp+oj7In3Jnzr0QO48+lDvr//5g9dgVvWL46xRe5VK9mYciWi0KJsyVpv1Hl2dDyF4Hei0mkk7cbPKDaJhTVunaDfDcGA9GvKjZUfElHynPK4QfK2frehDVLyV1lss+W6JbjmsvNdvy/p6hMnXqWSXnn6WiLA6kUXGZlnCIOBnCgDwk42+qkS8VIZddYTNOhWRtKfWd7pev20R7H1OkEBptWQtxVyU/63UlN+y7rFWP3tp0LVzpvA1ApRyqKkRqLWW9fbhrYibNBNa1vXCq+5A7+lkrVL+VcvughPvnJ8yserv/1U4se7VWMgJ0pR1AU0Ueutq3PaUw9Pnips0E2z+qReB+m3E3TK01d/vKP3SCrHu1VjICdKUdQFNCZGvNUbSD332kk8sv8o8iI4O1o2EnTrHbtm+gBkwF8HaWrRURZWoTKQE6UoahAwNeKtjDo3Lu/EPxffZ7zkz2lUG9cByIC/DtJU2icLq1AZyIlSZCIIRDnIwG1EnETJX1wHIAP+OkhTnWDa8wAAAzlRqkwFgTD11nGOiOsxtSeLG78dpInTfLKwCpWBnChFaQWBuEfE9cSdV67XQVZqvk3l5tM43q0aAzlRytIIAnGPiOvxGjHPaMlFzit7dZCVmm/TbyJJH+9WjYGcKAOSDgJpV1p4jZjPjpYx7/wZke/h1EFmoeY7DlzZSdSEvFZ0JlFp0dFWwB03LHP9+s337zGyLWylg7xl/WJsXN6JJ14+HmklbFYxkBM1oSzs9z341lnMaHHuTIqjY/jmzv3G9ytJ+00kLgzkRE0oC2dRDpw847qStFQGdu4dNL5fSZpvIqYO73DC/ciJmlia+33v6D2CrT39dXdfDHtavVONPADX/dLD3sfPfQ8cfdu1MinIBKvbfuQM5ERklN9l90PFElb+8y6cqRPI21vz2HLdkkCTwU418pXACcBIUPV9XwBjqnhnYm/3akE7Dx4sQUSxC7LIqKOtgE3XLsCdz3ifwuOVu3YbdderkY+j3NOrNt+NqVJPBnIiMiLUIiMfBze45a7dOo1N1y7wVSMfJHj6ecsIcsZnhakJVgZyIgvEtUugSWEWGfk5Ts2pisar0/jezw+hND2LMfk9QQOnn7eMoWIJj+4/6uu0pWqmJliz9f8EIpomzT1RgghT2lfvYIuZrc5VNF6dRk4ErXlgZGz6NwQNnH7eMvonJjJH3XoPD6ZKPVl+SJRhUc/kTFKY0j6nMsjWvKCQA774oXej97a1jp2VV6cxMqZwy3AEDZz13jIe3PP65M/HqeOoOKclh/bWXGylnhyRE2VY2nuiBBF2J8cwe83U293wc6sW4L7nD0feiKzeW8bPXj7hmRdvzQtaCjnce9MKLJl3XmylngzkRBlm00rEKDs5Bt1rpl6n8eU1C/HlNQsjB856HQagnnnx379iNr57wwcm7xtXp8tATpRhUQ+eSHqSNKmdHP12GlEDZ70OY83iudg98FvXn8/6912cyAIrLggiyrChYin0SkSvRTFZmiSNIomVqW7/jnfcsAy/OjGEf3rkAEYd8uOmVopW48pOIkuFCchROgCarrbDmHfBDNz8wz2TP5NqcXaYXNlJZKkw6QqbJkltUJ3D9+okCzng6+sW45PLLk20o2QgJ7JA0MlAmyZJbePVSbYW8mhrySX+tsM6cqIGlPbBEY0si52kkUAuIneLyHER2W/iekQUTRYOjmhUWewkTY3I7wWwztC1iCiiLBwc0aiy2Eka+Wmq6jMi0mXiWkRkRlI13c0mysKnuBgrP5wI5D2qepXL1zcD2AwAnZ2dyw4fPmzkvkREaUjjdKXUyw9VdTuA7cB4HXlS9yUiikPQSqI4sWqFiMhyDORERJYzVX74AIDnACwSkTdE5AsmrktERPWZqlr5rInrEBFRcEytEBFZjoGciMhyDORERJZjICcishwDORGR5RjIiYgsx0BORGQ5BnIiIssxkBMRWY6BnIjIcgzkRESWYyAnIrIcAzkRkeUYyImILMdATkRkOQZyIiLLMZATEVmOgZyIyHIM5ERElmMgJyKyHAM5EZHlGMiJiCzHQE5EZDkGciIiyzGQExFZjoGciMhyDORERJZjICcishwDORGR5RjIiYgsZySQi8g6EXlFRF4Vka+buCYREfkTOZCLSB7AdwGsB7AEwGdFZEnU6xIRkT8FA9dYAeBVVT0EACKyA8D1APoNXDuyoWIJPfsGMXDyDLpmzcSGpfPR0WbisYmIssFERLsEwOtVH78BYKWB60a2e+AUbrynF6rA8MgY2lvzuP3hftx70wos77ow7eYZxQ6LqHkl9psuIpsBbAaAzs7O2O83VCzhxnt6caY4Nvm54ZHxP994Ty96b12LmQ0S6JqpwyKi6UxMdv4awGVVH1868bkpVHW7qnaravecOXMM3NZbz75BqDp/TRXoeXEw9jaEMVQsYUfvEXzr0QPY0XsEQ8VS3e+vdFiVjmp4ZAxnimMTn/f++0RkPxND0t0AForI5RgP4J8B8GcGruvIbwph4OSZycBWa3hkDANvDsfVxNDCjKz9dFgbl8f/BkRE6YkcyFW1JCJfAvAYgDyAu1X1pcgtc+AW6O64YRkG3zo7Jbh3zZqJ9ta8YzBvb82ja3Z7HE0MLWwqKK4Oizl3InsY+c1U1UcAPGLiWm68At3n7u7FjJYczo6WpwR3EedriQAbrp4fZ3MDCzuyjqPDYs6dyC7WrOz0CnQAcHa0DOB3+eEv/rAPd9ywDDPb8mhvzQMYD2wz2/K496YVmZvoDDuy3rB0vmuHVSqX8cpvTvvKtVcw505kn2xFMw9egc7J8EgZ//Pam+i9dS16XhzEwJvD6Jrdjg1Xz89cEAfCj6w72gq496YVU0bQbYUciqUyBIK7/3sg0IiaOXci+1gzIq8EuiDuevZXAICNyztxy/rF2Li8M5NBHPAeWddLBS3vuhC9t67FluuW4AsfvHzy88XS1LcUPyNqGyeJiZqdNYHcK9B5yWqZYa3KyDpsKmhmWwEbl3di4dwO5HPO/1B+yi69Osyok8RBSyuJyJ9sDk8dOKUQZrTkcXbUPd0yOqZ47rWT01IBWa3IqIyso6SCoo6oNyydj9sfdt5dIcokMSdQieIj6jWDGJPu7m7t6+sL9XfPFEtTAt3BY6dx17MDrt/fWhD87zf+eDIYOgUUETRMQNnRewRbe/pdc+1brltSN8dt+t9oqFjCym27plQcVcxsyzfUKluiOInIHlXtrv28db89lRRCxVCxhPueP4KRiXxwrbzI5ASdbcv2w7w5mBhRm3gzqMYJVKJ4ZSdqhdTRVsD6qy7Gzr3Oud+zo+XJdIJNASVsKsIpBVU9ovYbjGs7zCg4gUoUL+sDOQCsevcs/PSl30zWklernqCzJaBEfXMwPaKOyrZVtkS2saZqxcuGpfORc6nUqE4nxFmRYZKJDb8qI+oslF1GKa0kovoaIpD7Ld2zJaDY8ubgV9TSSiLy1jC/QX7SCabyx3FrxFRE7c/n4vPPAaD42YFjeO34UGZKQIlsZF35oQm1JYxZW7Zvc7men0qbRi8BJYqLW/lhUwZyG9gY7Py02eZOiihtDVNH3iyyVnlSj99KG5tKQIlskc2oQADM1nLHzW+AbrSJXKIsaIiqFUqf3wBtSwkokU0YyDPIxl0C/QZoW0pAiWzCQJ4xuwdOYeW2Xdja0487nz6ErT39WLltF3YPnEq7aZ78BmjWlBOZx6qVDLG9oiNIpU3WS0CJsohVKxbo2TeI0phzx1oa08xXdASptLFpIpco6xjIM6CyiOb+Fw5PHs9Wq1gq45fHhhJuWXAM0ETJYyBPWW06wstvh0cSahUR2YSBPEVOi2i8vKu9NfD1s3ikHRGZxd/qFHktoqnVVshh4dyOyY/rBWmekUnUPBjIU+S1iKZWIS+TJXz1grRtR9oRUTSsI0+R1yKaitoa6+ogXQnOwyNjOFMcm/h8ycjBFERkDw7LUuR1UHJbIYdN1y7AwrkdU0r4/ARp7mdC1FwYyFNU76ALp1y2nyDdiAdTEJG7pgrkWaziCLpdrZ8g/SdXu4/0uZ8JUeNpmiX6Nh7U4MTvMv5GeV4i+p1YTggSkU8D+AcAVwJYoaq+onPSgdz2PUxq+Q3S3M+EqLHEtdfKfgCfAPAfEa8Tq0Y7lcZvOobL5YmaQ6RArqoHAEDc9i9NmFsO/OCx0w1XxcEgTUQVib1ni8hmAJsBoLMzeAAKu5LxlnWLcf8LR1yvyyqOqbI4IUxE3urmyEVkF4CLHb50m6runPiepwD8XVw58no5Ya8ceD025sjjwglSomwLnSNX1bXxNMkfP8vNg+xZUuuOP18WKYhnaQQbpS1c1k9kr8z/ZkZdyeilkBMcfets6LZlaWOqqG1ptAlhomYSaa8VEfm4iLwBYBWAh0XkMTPN+p0gKxmDKpU19ESnnz1PvP6uycOVo7SlwuSyfhsPjyayWdSqlYcAPGSoLY6irmT0MqMl/ERn2BFsHKP4qKPpoWIJx98uopATlMrTLxRkQjhLbylEzSLzux/6OZ3d62T22z52peu1c7nwy9XDjGDrjZyPv/1OqJFslNH07oFTWLltFx7df9QxiAP+l/WbeDMgouAynyOvt7FUZQLOa5HMoovPxV/9oA9jZUWprJjRkkcuN/XvBxVmYyqvkXNpTPGH//Ik8jkJPJINu0lWvROKZrTkUFZg7ZVz8ZN9g3UnT5lnJ0pH5gM5EH0l4x+9dw5+8Y2PGF2u7rUFrdsI1mvkXHvost+KkaFiCe+UyhgpOV/XazTtFXhzAoyMldGaz2Hn3kE83n+sbsfC7XOJ0mFFIAeir2Q0vRLS75tCNa+Rsxu/+faafqBuWwDvwFtWAAqcLY9f2E/Hwu1zidJhTSDPoqBb0HqN4t34ybfXaskLvr5+ET75gcs83zpMdyxh3lKIKLrMT3ZmXWWkf8v6xdi4vNMzcLpNyrYWBG0F5x9FmHy7AFCVuqkjr4lkN14pEq9J5yjzEUTkjb9ZCXMaxa9edBFWf/spOBV1hMm3j4wpbu95CUvmn+c5UeqWHhqbqF6pzdsD9VMkQd9SiCi6pjlYIuuC7nOyo/cItvb0e6ZF/O4jU7tveaVjaZT924kaRSwHS4TFQO4syEEQfjYKa2/NY8t1S0JN8nIDLaLsietgiYaRhc2v6lXW1LbxjhuW4Qvf343RMefOOErJH1MkRPbgbyXsWFbuNkL+3KoFuO+5wxhxCOZRS/54eAWRHZq+aiXqsvIkNojyauMDvUfQknf+MbLkj6g5NP2IPMqy8qRG8t77rQs2rVqA+54/7HthkpMspJaIKJym/00Nu6w8yYMY6rVRIJHy2TaklojIXdMH8rDLypPcIMpPG8Pms3kyEJH9mjZHXsltHzx2GqXy9IUvgHeOOckNojy38kW0PLifDomIsq0ph1q1qYTx5fGKtkIOxVI5VI65mukNoiorMDfd9QLeGZ3a6Yypov/o26FTINyxkMh+TRfInVIJ1UvR//IPLsfCuR11F+P84LkB13sEHSX7mWi8ct55yDmMyt8ZLUdKgXDHQiL7NV0g90ol5HOChXM76uaae/YNYjxcO9u0aoHxiUave/rNyTt1GNyxkMh+TRfITaQSvK4BAOIR5KsFmWiM2m6vDiPovupElC1N91tqIpVgKh0RpPIlyj39dBhcjk9kr6arWvFzmHMS1wCCjbKj3NNPhxFkX3UiypamC+QmDj8wdYBCZZTtpHaUHeWerEwhamxNOewysbOfiWsEnWgMe09WphA1tqYM5ICZnf1MXGPTtQvwvZ8fQk4EI2Nad6IxzD1ZmULU2Jo2kKetuoqkVAZa8+OHJn9u1QJ8ec1CozlqtyPdkqxM4aZcRPHhCUEp8DrdJ86j1IKcQGQSTxsiMoMnBGVIkhtuVUvjoAhuykUUP/4GGeYnhRCkimSoWMKDfa/jiVeOAwBWL56LTy27FACsSFWk1WkRNZPs/eZbzO9ye79VJLsHTk3bKOvpg29i2yMHkBMgJ5L5/cNZ+kgUv6arI49LkCPj/CzuGSqW8Bd3T9/tEABGSmW8M1oOdTRdmOeKcpRdkFp5IgonUiAXkX8VkZdF5EUReUhELjDVMNsE2dfbz+Kenn2DKDkcqOzF9P7huwdOYeW2Xdja0487nz6ErT39WLltF3YPnPJ9DVOrYInIXdQR+eMArlLVqwEcBPD30Ztkp6AphMrini3XLcHNH7oCW65bgt5b106mRgZOnsFIwEBuMlUR9VDqClOrYInIXaTfIlX9adWHzwP4VLTm2CvM6kmvKpKuWTPRmpdAwdxkqsLkJKWJVbBE5M5kjvzzAB51+6KIbBaRPhHpO3HihMHbZoPpFMKGpfNRyPvbDjfKfdyYnqTkplxE8akbyEVkl4jsd/jv+qrvuQ1ACcD9btdR1e2q2q2q3XPmzDHT+gwxnULoaCvg+59fiXNapv+IWgs5nNOSizVVwUlKIntEXtkpIjcC+GsAH1ZVX8O0Rl7ZaXr15JliCQ/+4nU8cWD8LWbN4ovwyUodeYypirRWnxKRO7eVnZECuYisA/AdAB9SVd/5kkYO5I2ES+uJsiWuQP4qgDYAJyc+9byqfrHe32Mgt0da+7MQ0XSx7LWiqu+J8vcp+9LYn4WIguHKTiIiyzGQExFZjoGciMhyDORERJZL5YQgETkB4HCdb5sN4M0EmpNFzfrszfrcQPM+e7M+NxDu2Reo6rQVlakEcj9EpM+pzKYZNOuzN+tzA8377M363IDZZ2dqhYjIcgzkRESWy3Ig3552A1LUrM/erM8NNO+zN+tzAwafPbM5ciIi8ifLI3IiIvKBgZyIyHKZDuTNeriziHxaRF4SkbKINEVploisE5FXRORVEfl62u1JiojcLSLHRWR/2m1JkohcJiJPikj/xP/X/zbtNiVBRM4RkV4R2Tfx3P9o4rqZDuRo3sOd9wP4BIBn0m5IEkQkD+C7ANYDWALgsyKyJN1WJeZeAOvSbkQKSgC+qqpLAFwL4G+a5GdeBLBGVZcCuAbAOhG5NupFMx3IVfWnqlo5rv15AJem2Z6kqOoBVX0l7XYkaAWAV1X1kKqOANgB4Po6f6chqOozAE6l3Y6kqepRVf3FxJ9PAzgA4JJ0WxU/HTc08WHLxH+RK04yHchreB7uTFa7BMDrVR+/gSb4paZxItIF4P0AXki3JckQkbyI7AVwHMDjqhr5uVM/6kVEdgG42OFLt6nqzonvqXu4s238PDdRoxORDgAPAviKqr6ddnuSoKpjAK6ZmPN7SESuUtVIcySpB3JVXev19YnDnTdg/HDnhil6r/fcTebXAC6r+vjSic9RAxORFowH8ftV9UdptydpqvqWiDyJ8TmSSIE806mVicOdvwbgT1V1OO32UGx2A1goIpeLSCuAzwD4ccptohiJiAC4C8ABVf1O2u1JiojMqVTficgMAB8B8HLU62Y6kAP4dwDnAnhcRPaKyJ1pNygJIvJxEXkDwCoAD4vIY2m3KU4TE9pfAvAYxie9/lNVX0q3VckQkQcAPAdgkYi8ISJfSLtNCfkggE0A1kz8bu8VkY+l3agEzAPwpIi8iPEBzOOq2hP1olyiT0RkuayPyImIqA4GciIiyzGQExFZjoGciMhyDORERJZjICcishwDORGR5f4f+5QX8ry3on8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.310326 -0.538983  0.522009 -0.630752  0.888074\n",
            "1  0.026933  1.005510 -1.519784  0.317596  1.706252\n",
            "2  1.454472 -1.948507  0.989502  1.673824  2.006770\n",
            "3  0.299680 -1.090324 -0.968199  0.285824  0.376355\n",
            "4  1.568637  0.042656 -0.204593  1.126121  2.629879\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.408410  0.615359 -2.553963  1.017630  0.665036\n",
            "96  1.271942  0.739803  1.451475 -2.180999  1.219121\n",
            "97 -1.462029 -1.407781  0.657375  0.320367  0.309101\n",
            "98 -0.355275 -1.795455  0.762725 -0.701842 -0.118037\n",
            "99 -0.845194 -0.817530 -1.009229  0.328676 -0.005031\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.012668   -0.098384    0.002392    0.037210    1.007865\n",
            "std      0.984979    1.056384    0.890788    1.027167    0.875303\n",
            "min     -2.174584   -2.241255   -2.553963   -3.162631   -0.881631\n",
            "25%     -0.761235   -0.900435   -0.565406   -0.652427    0.344661\n",
            "50%     -0.013255   -0.008498   -0.023229    0.078830    1.035789\n",
            "75%      0.724811    0.740714    0.684799    0.733959    1.709912\n",
            "max      2.259437    2.192720    2.373255    2.320635    2.909347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yIvruqbEdNg",
        "colab_type": "text"
      },
      "source": [
        "Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLKbFU9CEdYL",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAgXrvNhFcwD",
        "colab_type": "code",
        "outputId": "e85dd482-f177-4dd0-a443-5b7575d06b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09766862225254742 0.1827415524841143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZxbTuSOEp1j",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7Mr9tbFg-6",
        "colab_type": "code",
        "outputId": "ea25bd07-85c4-41c1-ec62-a4cca17d0fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.30250249211695207\n",
            "0.30246767315723116\n",
            "0.30243335732066357\n",
            "0.3023995359985447\n",
            "0.3023662007767267\n",
            "0.30233334342983526\n",
            "0.3023009559156906\n",
            "0.3022690303699237\n",
            "0.3022375591007821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QUCLPlVEuuL",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression using L1 Regulrisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv1xezoKFl72",
        "colab_type": "code",
        "outputId": "acf7af28-f971-4800-c3a8-08cf560fdc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08856264127762674 0.18275259655444528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLMDVi72E61C",
        "colab_type": "text"
      },
      "source": [
        "Linear regression using L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpfw1ZXXFpTh",
        "colab_type": "code",
        "outputId": "8485e5c5-f24c-4ed5-cf79-46ffa11b278f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09720105910595599 0.1827419335908878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gZvGxSFJoD",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression using L1 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEKk3XIeFs34",
        "colab_type": "code",
        "outputId": "e4c24df1-9ccf-4e47-b7cc-fe4495842b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "-0.0957178147843501\n",
            "-0.49126928267553843\n",
            "-0.8838759664840703\n",
            "-1.2735829336825866\n",
            "-1.660434584454674\n",
            "-2.044474635211784\n",
            "-2.4257461061958714\n",
            "-2.804291312728332\n",
            "-3.1801518596965117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozhNbzapFQzA",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression using L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Sa5r6QFwTu",
        "colab_type": "code",
        "outputId": "303b803e-2ed2-4af6-e029-dafdadc0b5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.30253765266491595\n",
            "0.30260591111172064\n",
            "0.30273910226152506\n",
            "0.30293387149915413\n",
            "0.3031870022514592\n",
            "0.30349541047524015\n",
            "0.30385613937861167\n",
            "0.30426635436466415\n",
            "0.30472333818688496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761OTVJ6FYL8",
        "colab_type": "text"
      },
      "source": [
        "K means clustering algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF3iQiGZFzf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PStCKV5zF4NF",
        "colab_type": "code",
        "outputId": "32c6574d-3f3c-4a2f-c87d-1320a6007469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "155.2251271407831\n",
            "167.0476488109652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qr1Azy0Fdsw",
        "colab_type": "text"
      },
      "source": [
        "Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrzqzNpWFful",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNkDve9wF-yS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E3sqn-6FoIe",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewDV2yxCFudG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjtp6HBDFzfO",
        "colab_type": "text"
      },
      "source": [
        "K-means clustering using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJPOWQtMF6Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}